{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Evaluation (Prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial approximation, $v_0$, is chosen arbitrarily\n",
    "(except that the terminal state, if any, must be given value 0), and each successive approximation is obtained by using the Bellman expectation equation for $v_{\\pi}$ as an update rule:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "v_{k+1}(s) \\doteq  {\\mathbb{E}}_{\\pi} [R_{t+1} + \\gamma v_k(s')| S_t = s] = \\sum_{a}^{} \\pi(a|s) \\sum_{s', r}^{} p(s', r \\ |s, a) [r + \\gamma v_k(s')]\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for all $s \\in S$. Clearly, $v_k = v_{\\pi}$ is a fixed point for this update rule because the Bellman equation for $v_{\\pi}$ assures us of equality in this case. Indeed, the sequence $v_k$  can be shown in general to converge to $v_{\\pi}$ as $k \\rightarrow \\infty$ under the same conditions that guarantee the existence of $v_{\\pi}$ . This algorithm is called iterative policy evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formally, iterative policy evaluation converges only in the limit, but in practice it must be halted short of this. As soon as $\\Delta = \\max_{s \\in S}|v_{k+1}(s)-v_k(s)|$ is less than threshold $\\theta$ the evaluation terminates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Input $\\pi$, the policy to be evaluated <br>\n",
    "- a small threshold $\\theta > 0$ determining accuracy of estimation Initialize $V(s)$, for all $s \\in S+$, arbitrarily except that $V(\\text{terminal}) = 0$ <br>\n",
    "- Loop: <br>\n",
    "    + $\\Delta \\leftarrow 0$\n",
    "    + Loop for each$s \\in S$\n",
    "        + $v \\leftarrow V(s)$\n",
    "        + $V(s) \\leftarrow \\sum_{a}^{} \\pi(a|s) \\sum_{s', r}^{} p(s', r \\ |s, a) [r + \\gamma V(s')]$\n",
    "        + $\\Delta = max(\\Delta, |v-V(s)|)$\n",
    "<br>\n",
    "- Until $\\Delta > \\theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 4.1 Grid World"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0 -14.0 -20.0 -22.0]\n",
      " [-14.0 -18.0 -20.0 -20.0]\n",
      " [-20.0 -20.0 -18.0 -14.0]\n",
      " [-22.0 -20.0 -14.0 0.0]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.1f}\".format(x)})\n",
    "\n",
    "actions = [(0, -1), (0, 1), (1, 0), (-1, 0)]\n",
    "gamma = 1\n",
    "theta = 0.01\n",
    "policy = 0.25\n",
    "\n",
    "def p(state, action):\n",
    "    \n",
    "    next_state = (state[0] + action[0], state[1] + action[1])\n",
    "    if ((next_state[0] >= 0 and next_state[0] < 4) and\n",
    "        (next_state[1] >= 0 and next_state[1] < 4)):\n",
    "        return next_state, -1\n",
    "    return state, -1 # wall\n",
    "    \n",
    "v = np.zeros((4, 4))\n",
    "while True:\n",
    "    delta = 0 \n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            if (i,j) == (0,0) or (i,j) == (3,3):\n",
    "                continue\n",
    "            v_prev = v[i, j]\n",
    "            tmp = 0\n",
    "            for a in actions:\n",
    "                n_s, r = p((i, j), a)\n",
    "                tmp += policy * (r + gamma * v[n_s[0], n_s[1]])\n",
    "            v[i, j] = tmp\n",
    "            delta = max(delta, abs(v[i, j]-v_prev))\n",
    "    if delta < theta:\n",
    "        break\n",
    "print(np.round(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4.1 In Example 4.1, if $\\pi$ is the equiprobable random policy, what is $q_{\\pi}(11,down)$? What is $q_{\\pi}(7,down)$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer : $r(s,a,s')=-1$ for all s,s' and actions, and this is an undiscounted episodic task. values of $v_{\\infty}$ are from figure 4.2 in book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "q_{\\pi}(s, a) = \\sum_{s', r}^{} p(s',r{\\space}|s, a)[r(s,a,s') + {\\gamma}v_{\\pi}(s')]\n",
    "\\end{align*}\n",
    "\\begin{align*}\n",
    "q_{\\pi}(11, \\text{down}) = -1 + 1\\times0 = -1\n",
    "\\end{align*}\n",
    "\\begin{align*}\n",
    "q_{\\pi}(7, \\text{down}) = -1 + 1\\times(-14) = -15\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4.2 In Example 4.1, suppose a new state 15 is added to the gridworld just below state 13, and its actions, left, up, right, and down, take the agent to states 12, 13, 14, and 15, respectively. Assume that the transitions from the original states are unchanged. What, then, is $v_{\\pi}(15)$ for the equiprobable random policy? Now suppose the dynamics of state 13 are also changed, such that action down from state 13 takes the agent to the new state 15. What is $v_{\\pi}(15)$ for the equiprobable random policy in this case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- unchanged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "v_0(15) = 0\n",
    "\\end{align*}\n",
    "\\begin{align*}\n",
    "v_{1}(15) = 0.25 \\times [(-21) + (-23) + (-15) + (-1)] = -15.0\n",
    "\\end{align*}\n",
    "\\begin{align*}\n",
    "v_{2}(15) = 0.25 \\times [(-21) + (-23) + (-15) + (-16)] = -18.75\n",
    "\\end{align*}\n",
    "\\begin{align*}\n",
    "...\n",
    "\\end{align*}\n",
    "\\begin{align*}\n",
    "v_{\\infty}(15) = -20\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 13 is also changing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "v_0(15) = 0\n",
    "\\end{align*}\n",
    "\\begin{align*}\n",
    "v_0(13) = -20\n",
    "\\end{align*}\n",
    "\\begin{align*}\n",
    "v_{1}(15) = 0.25 \\times [(-21) + (-23) + (-15) + (-1)] = -15\n",
    "\\end{align*}\n",
    "\\begin{align*}\n",
    "v_{1}(13) = 0.25 \\times [(-21) + (-23) + (-15) + (-16)] = -18.75\n",
    "\\end{align*}\n",
    "\\begin{align*}\n",
    "v_{1}(15) = 0.25 \\times [(-19.75) + (-23) + (-15) + (-16)] = -18.4375\n",
    "\\end{align*}\n",
    "\\begin{align*}\n",
    "v_{1}(13) = 0.25 \\times [(-19.75) + (-23) + (-15) + (-19.4375)] = -19.296875\n",
    "\\end{align*}\n",
    "\\begin{align*}\n",
    "...\n",
    "\\end{align*}\n",
    "\\begin{align*}\n",
    "v_{\\infty}(13) = -19.5\n",
    "\\end{align*}\n",
    "\\begin{align*}\n",
    "v_{\\infty}(15) = -19.5 \n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4.3 What are the equations analogous to (4.3), (4.4), and (4.5) for the action- valuefunction $q_{\\pi}$ and its successive approximation by a sequence of functions $q_0$,$q_1$,$q_2$,...?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "q_{k+1}(s, a) = \\sum_{s', r}^{} p(s',r{\\space}|s, a)[r(s,a,s') + {\\gamma}\\sum_{a'}^{}\\pi(a|s)q_{k}(s', a')]\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know $v_{\\pi}$, let's try to find better policies. We can choose an action a in some state s that is not in our policy and thereafter following the existing policy, $\\pi$ (we call this new policy $\\pi'$). If this choice is better than following $\\pi$ all the  time $q_{\\pi}(s,\\pi'(s)) \\ge v_{\\pi}(s)$ than we can say that this new policy obtain greater or equal expected return from all states $s \\in S$. In other word $v_{\\pi'}(s) \\ge v_{\\pi}(s)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have seen how, given a policy and its value function, we can easily evaluate a change in the policy at a single state to a particular action. It is a natural extension to consider changes at all states and to all possible actions, selecting at each state the action that appears best according to $q_{\\pi}(s,a)$. In other words, to consider the new greedy policy, $\\pi'$, given by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "\\pi'(s) = \\text{argmax}_a q_{\\pi}(s, a)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The greedy policy takes the action that looks best in the short term—after one step of lookahead—according to $v_{\\pi}$. so we know that it is as good as, or better than, the original policy. The process of making a new policy that improves on an original policy, by making it greedy with respect to the value function of\n",
    "the original policy, is called policy improvement. If new policy is as good as old one then $v_{\\pi}$ and $v_{\\pi'}$ are both optimal policy $v_{*}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "v_{*}(s) = v_{\\pi}(s) = v_{\\pi'}(s) = \\max_a q_{\\pi}(s, a)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once a policy, $\\pi$, has been improved using $v_{\\pi}$ to yield a better policy, $\\pi'$, we can then compute $v_{\\pi'}$ and improve it again to yield an even better $\\pi''$. Each policy is guaranteed to be a strict improvement over the previous one (unless it is already optimal). Because a finite MDP has only a finite number of policies, this process must converge to an optimal policy and optimal value function in a finite number of iterations. This way of finding an optimal policy is called policy iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Initialization\n",
    "   - $V(s) \\in \\mathbb{R}$ and $\\pi(s) \\in A(s)$ arbitrarily for all $s \\in S$ <br>\n",
    "<br>\n",
    "2. Policy Evaluation\n",
    "    - Loop: <br>\n",
    "        + $\\Delta \\leftarrow 0$\n",
    "        + Loop for each$s \\in S$\n",
    "            + $v \\leftarrow V(s)$\n",
    "            + $V(s) \\leftarrow \\sum_{a}^{} \\pi(a|s) \\sum_{s', r}^{} p(s', r \\ |s, a) [r + \\gamma V(s')]$\n",
    "            + $\\Delta = max(\\Delta, |v-V(s)|)$\n",
    "    <br>\n",
    "    - Until $\\Delta > \\theta$\n",
    "<br><br>\n",
    "3. Policy Improvement\n",
    "    - policy_stable $\\leftarrow$ True\n",
    "    - Loop for each$s \\in S$\n",
    "        + old_action $\\leftarrow \\pi(s)$\n",
    "        + $\\pi(s) \\leftarrow \\text{argmax}_a \\sum_{s', r}^{} p(s',r{\\space}|s, a)[r(s,a,s') + {\\gamma}v_{k}(s')]$\n",
    "        + if old_action $\\neq \\pi(s)$ then policy_stable $\\leftarrow$ False\n",
    "    - if policy_stable then stop and return $V \\approx v_{*}$ and $\\pi \\approx \\pi_{*}$ else goto 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 4.2 Jack’s Car Rental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [02:34<10:17, 154.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration#1 -> 215 policies changed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 2/5 [06:21<09:32, 190.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration#2 -> 147 policies changed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 3/5 [09:16<06:10, 185.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration#3 -> 9 policies changed.\n",
      "----------\n",
      "V*\n",
      "[[132.1 140.6 148.6 155.8 162.4 168.8 175.0 180.9 186.2 191.2 195.8 199.9\n",
      "  203.8 207.2 210.3 213.1 215.6 217.9 219.9 221.8 223.5]\n",
      " [140.7 149.2 157.2 164.4 170.8 177.0 182.9 188.2 193.2 197.8 201.9 205.8\n",
      "  209.2 212.3 215.1 217.8 220.1 222.3 224.2 225.9 227.5]\n",
      " [149.0 157.5 165.6 172.8 179.0 184.9 190.2 195.2 199.8 203.9 207.8 211.2\n",
      "  214.3 217.1 219.8 222.1 224.3 226.2 227.9 229.5 230.9]\n",
      " [156.9 165.4 173.4 180.6 186.9 192.2 197.2 201.8 205.9 209.8 213.2 216.3\n",
      "  219.1 221.8 224.1 226.3 228.2 229.9 231.5 232.9 234.2]\n",
      " [164.0 172.5 180.5 187.7 193.9 199.2 203.8 207.9 211.8 215.2 218.3 221.1\n",
      "  223.8 226.1 228.3 230.2 231.9 233.5 234.9 236.2 237.3]\n",
      " [170.5 178.8 186.8 194.0 200.1 205.4 209.9 213.8 217.2 220.3 223.1 225.8\n",
      "  228.1 230.3 232.2 233.9 235.5 236.9 238.2 239.3 240.3]\n",
      " [176.8 184.8 192.3 199.4 205.5 210.7 215.1 218.9 222.2 225.1 227.8 230.1\n",
      "  232.3 234.2 235.9 237.5 238.9 240.2 241.3 242.3 243.2]\n",
      " [182.8 190.3 197.4 204.2 210.2 215.3 219.6 223.2 226.4 229.2 231.7 233.9\n",
      "  235.9 237.7 239.3 240.8 242.1 243.2 244.3 245.2 245.9]\n",
      " [188.3 195.4 202.2 208.4 214.3 219.2 223.4 226.9 230.0 232.6 235.0 237.1\n",
      "  239.0 240.6 242.1 243.4 244.7 245.7 246.7 247.5 248.1]\n",
      " [193.4 200.2 206.4 212.3 217.9 222.7 226.7 230.1 233.0 235.6 237.8 239.8\n",
      "  241.5 243.1 244.5 245.7 246.8 247.8 248.7 249.4 250.0]\n",
      " [198.2 204.4 210.3 215.9 221.1 225.8 229.7 232.9 235.7 238.2 240.3 242.2\n",
      "  243.8 245.3 246.6 247.7 248.7 249.6 250.4 251.1 251.6]\n",
      " [202.4 208.3 213.9 219.1 224.0 228.6 232.3 235.4 238.1 240.5 242.5 244.3\n",
      "  245.8 247.2 248.4 249.5 250.4 251.2 252.0 252.6 253.0]\n",
      " [206.3 211.9 217.1 222.0 226.6 231.0 234.6 237.6 240.2 242.5 244.4 246.1\n",
      "  247.6 248.8 250.0 251.0 251.9 252.6 253.3 253.9 254.3]\n",
      " [209.9 215.1 220.0 224.6 229.0 233.3 236.7 239.6 242.1 244.2 246.1 247.7\n",
      "  249.1 250.3 251.4 252.3 253.1 253.9 254.5 255.0 255.4]\n",
      " [213.1 218.0 222.6 227.0 231.3 235.3 238.6 241.3 243.7 245.7 247.5 249.1\n",
      "  250.4 251.6 252.6 253.5 254.2 254.9 255.5 256.0 256.3]\n",
      " [216.0 220.6 225.0 229.3 233.3 237.1 240.2 242.8 245.1 247.1 248.8 250.3\n",
      "  251.5 252.7 253.6 254.5 255.2 255.8 256.4 256.8 257.2]\n",
      " [218.6 223.0 227.3 231.3 235.1 238.7 241.7 244.2 246.4 248.3 249.9 251.3\n",
      "  252.5 253.6 254.5 255.3 256.0 256.6 257.1 257.5 257.9]\n",
      " [221.0 225.3 229.3 233.1 236.7 240.1 243.1 245.4 247.5 249.3 250.9 252.2\n",
      "  253.4 254.4 255.3 256.0 256.7 257.3 257.8 258.2 258.5]\n",
      " [223.3 227.3 231.1 234.7 238.1 241.4 244.2 246.5 248.5 250.2 251.7 253.0\n",
      "  254.1 255.1 255.9 256.7 257.3 257.8 258.3 258.7 259.0]\n",
      " [225.3 229.1 232.7 236.1 239.4 242.4 245.2 247.3 249.3 251.0 252.4 253.7\n",
      "  254.7 255.7 256.5 257.2 257.8 258.3 258.7 259.1 259.4]\n",
      " [227.1 230.7 234.1 237.4 240.4 243.3 245.9 248.0 249.9 251.5 252.9 254.1\n",
      "  255.2 256.1 256.9 257.6 258.1 258.6 259.1 259.4 259.7]]\n",
      "----------\n",
      "Policy\n",
      "[[ 0  0  0  0 -1 -2 -2 -3 -3 -4 -4 -5 -5 -5 -5 -5 -5 -5 -5 -5 -5]\n",
      " [ 0  0  0  0 -1 -1 -2 -2 -3 -3 -4 -4 -4 -4 -5 -5 -5 -5 -5 -5 -5]\n",
      " [ 0  0  0  0  0 -1 -1 -2 -2 -3 -3 -3 -3 -4 -4 -4 -4 -4 -4 -4 -4]\n",
      " [ 0  0  0  0  0  0 -1 -1 -2 -2 -2 -2 -3 -3 -3 -3 -3 -3 -3 -3 -3]\n",
      " [ 0  0  0  0  0  0  0 -1 -1 -1 -1 -2 -2 -2 -2 -2 -2 -2 -2 -2 -2]\n",
      " [ 1  0  0  0  0  0  0  0  0  0 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      " [ 1  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 -1]\n",
      " [ 2  1  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 2  2  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 3  2  1  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 3  2  2  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 3  3  2  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 4  3  2  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 4  3  2  1  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 4  3  2  2  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 4  3  3  2  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 4  4  3  2  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 5  4  3  2  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 5  4  3  2  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 5  4  3  2  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 5  4  3  2  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from math import factorial\n",
    "from tqdm import tqdm\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.1f}\".format(x)})\n",
    "\n",
    "actions = np.arange(-5, 6)\n",
    "gamma = 0.9\n",
    "theta = 1e-4\n",
    "policy = np.zeros((21, 21), dtype=int)\n",
    "\n",
    "cache = {}\n",
    "def poisson(n, l):\n",
    "    global cache\n",
    "    if (n, l) not in cache:\n",
    "        cache[(n,l)] = ((l**n)*(np.exp(-l)))/factorial(n)\n",
    "    return cache[(n,l)]\n",
    "\n",
    "#poisson for lambda=4 is mostly in range 0-9 / lambda=3 in range 0-7 / lambda=2 in range 0-5\n",
    "outcomes = [(i, j, m, n) for i in range(9) for j in range(7) for m in range(7) for n in range(5)]\n",
    "def EG(state, action, v):\n",
    "    \n",
    "    returns = -2 * abs(action)\n",
    "    ret_fir, ret_sec = 3, 2\n",
    "    for (rent_fir, rent_sec, ret_fir, ret_sec) in outcomes:\n",
    "        new_state = (min(state[0] - action, 20), min(state[1] + action, 20))\n",
    "        reward = 10*(min(new_state[0], rent_fir) + min(new_state[1], rent_sec))\n",
    "        new_state = (new_state[0] - min(new_state[0], rent_fir), new_state[1] - min(new_state[1], rent_sec))\n",
    "        prob = poisson(rent_fir, 4) * poisson(rent_sec, 3) * poisson(ret_fir, 3) * poisson(ret_sec, 2)\n",
    "        new_state = (min(new_state[0] + ret_fir, 20), min(new_state[1] + ret_sec, 20))\n",
    "        returns += prob * (reward + gamma * v[new_state[0], new_state[1]])\n",
    "\n",
    "    return returns\n",
    "\n",
    "value = np.zeros((21, 21))\n",
    "states = [(i, j) for i in range(21) for j in range(21)]\n",
    "iteration = 5\n",
    "for i in tqdm(range(iteration)):\n",
    "\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for state in states:\n",
    "            v_prev = v[state[0], state[1]]\n",
    "            v[state[0], state[1]] = EG((state[0], state[1]), policy[state[0], state[1]], v)\n",
    "            delta = max(delta, abs(v[state[0], state[1]]-v_prev))\n",
    "        if delta < theta:\n",
    "            break\n",
    "\n",
    "    policy_stable = True\n",
    "    policy_changes = 0\n",
    "    for state in states:\n",
    "        action_returns = []\n",
    "        old_action = policy[state[0], state[1]]\n",
    "        for action in actions:\n",
    "            if (0 <= action <= state[0]) or (state[1] >= abs(action) > 0):\n",
    "                action_returns.append(EG((state[0], state[1]), action, v))\n",
    "            else:\n",
    "                action_returns.append(-np.Infinity)\n",
    "        policy[state[0], state[1]] = actions[np.argmax(action_returns)]\n",
    "        if old_action != policy[state[0], state[1]]:\n",
    "            policy_stable = False\n",
    "            policy_changes += 1\n",
    "    \n",
    "    if policy_stable:\n",
    "        break\n",
    "    print('iteration#{} -> {} policies changed.'.format(i+1, policy_changes))\n",
    "    \n",
    "\n",
    "print('-'*10)\n",
    "print('V*')\n",
    "print(v)\n",
    "print('-'*10)\n",
    "print('Policy')\n",
    "print(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4.4 The policy iteration algorithm on page 80 has a subtle bug in that it may never terminate if the policy continually switches between two or more policies that are equally good. This is ok for pedagogy, but not for actual use. Modify the pseudocode so that convergence is guaranteed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: We can check the sum of all state values that make the optimal policy, ($\\sum_{s}^{} v_{\\pi'}(s) = \\sum_{s}^{}\\max_a q_{\\pi}(s, a)$) if this sum converges then we got our optimal policy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4.5 How would policy iteration be defined for action values? Give a complete algorithm for computing q⇤, analogous to that on page 80 for computing v⇤. Please pay special attention to this exercise, because the ideas involved will be used throughout the rest of the book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4.6 Suppose you are restricted to considering only policies that are \"-soft, meaning that the probability of selecting each action in each state, s, is at least \"/|A(s)|. Describe qualitatively the changes that would be required in each of the steps 3, 2, and 1, in that order, of the policy iteration algorithm for v⇤ on page 80."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4.7 (programming) Write a program for policy iteration and re-solve Jack’s car rental problem with the following changes. One of Jack’s employees at the first location rides a bus home each night and lives near the second location. She is happy to shuttle one car to the second location for free. Each additional car still costs \\$2, as do all cars moved in the other direction. In addition, Jack has limited parking space at each location. If more than 10 cars are kept overnight at a location (after any moving of cars), then an additional cost of $4 must be incurred to use a second parking lot (independent of how many cars are kept there). These sorts of nonlinearities and arbitrary dynamics often occur in real problems and cannot easily be handled by optimization methods other than dynamic programming. To check your program, first replicate the results given for the original problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 4.3 Gambler’s Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4.8 Why does the optimal policy for the gambler’s problem have such a curious form? In particular, for capital of 50 it bets it all on one flip, but for capital of 51 it does not. Why is this a good policy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4.9 (programming) Implement value iteration for the gambler’s problem and solve it for ph = 0.25 and ph = 0.55. In programming, you may find it convenient to introduce two dummy states corresponding to termination with capital of 0 and 100, giving them values of 0 and 1 respectively. Show your results graphically, as in Figure 4.3. Are your results stable as ✓ ! 0?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4.10 What is the analog of the value iteration update (4.10) for action values, qk+1(s, a)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Asynchronous Dynamic Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generalized Policy Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Efficiency of Dynamic Programming"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
