{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Evaluation (Prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial approximation, $v_0$, is chosen arbitrarily\n",
    "(except that the terminal state, if any, must be given value 0), and each successive approximation is obtained by using the Bellman expectation equation for $v_{\\pi}$ as an update rule:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "v_{k+1}(s) \\doteq  {\\mathbb{E}}_{\\pi} [R_{t+1} + \\gamma v_k(s')| S_t = s] = \\sum_{a}^{} \\pi(a|s) \\sum_{s', r}^{} p(s', r \\ |s, a) [r + \\gamma v_k(s')]\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for all $s \\in S$. Clearly, $v_k = v_{\\pi}$ is a fixed point for this update rule because the Bellman equation for $v_{\\pi}$ assures us of equality in this case. Indeed, the sequence $v_k$  can be shown in general to converge to $v_{\\pi}$ as $k \\rightarrow \\infty$ under the same conditions that guarantee the existence of $v_{\\pi}$ . This algorithm is called iterative policy evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formally, iterative policy evaluation converges only in the limit, but in practice it must be halted short of this. As soon as $\\Delta = \\max_{s \\in S}|v_{k+1}(s)-v_k(s)|$ is less than threshold $\\theta$ the evaluation terminates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Input $\\pi$, the policy to be evaluated <br>\n",
    "- a small threshold $\\theta > 0$ determining accuracy of estimation Initialize $V(s)$, for all $s \\in S+$, arbitrarily except that $V(\\text{terminal}) = 0$ <br>\n",
    "- Loop: <br>\n",
    "    + $\\Delta \\leftarrow 0$\n",
    "    + Loop for each$s \\in S$\n",
    "        + $v \\leftarrow V(s)$\n",
    "        + $V(s) \\leftarrow \\sum_{a}^{} \\pi(a|s) \\sum_{s', r}^{} p(s', r \\ |s, a) [r + \\gamma V(s')]$\n",
    "        + $\\Delta = max(\\Delta, |v-V(s)|)$\n",
    "<br>\n",
    "- Until $\\Delta > \\theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 4.1 Grid World"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0 -14.0 -20.0 -22.0]\n",
      " [-14.0 -18.0 -20.0 -20.0]\n",
      " [-20.0 -20.0 -18.0 -14.0]\n",
      " [-22.0 -20.0 -14.0 0.0]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.1f}\".format(x)})\n",
    "\n",
    "actions = [(0, -1), (0, 1), (1, 0), (-1, 0)]\n",
    "gamma = 1\n",
    "theta = 0.01\n",
    "policy = 0.25\n",
    "\n",
    "def p(state, action):\n",
    "    \n",
    "    next_state = (state[0] + action[0], state[1] + action[1])\n",
    "    if ((next_state[0] >= 0 and next_state[0] < 4) and\n",
    "        (next_state[1] >= 0 and next_state[1] < 4)):\n",
    "        return next_state, -1\n",
    "    return state, -1 # wall\n",
    "    \n",
    "v = np.zeros((4, 4))\n",
    "m = 0\n",
    "while True:\n",
    "    delta = 0\n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            if (i,j) == (0,0) or (i,j) == (3,3):\n",
    "                continue\n",
    "            v_prev = v[i, j]\n",
    "            tmp = 0\n",
    "            for a in actions:\n",
    "                n_s, r = p((i, j), a)\n",
    "                tmp += policy * (r + gamma * v[n_s[0], n_s[1]])\n",
    "            v[i, j] = tmp\n",
    "            delta = max(delta, abs(v[i, j]-v_prev))\n",
    "    if delta < theta:\n",
    "        break\n",
    "print(np.round(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4.1 In Example 4.1, if $\\pi$ is the equiprobable random policy, what is $q_{\\pi}(11,down)$? What is $q_{\\pi}(7,down)$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer : $r(s,a,s')=-1$ for all s,s' and actions, and this is an undiscounted episodic task. values of $v_{\\infty}$ are from figure 4.2 in book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "q_{\\pi}(s, a) = \\sum_{s', r}^{} p(s',r{\\space}|s, a)[r(s,a,s') + {\\gamma}v_{\\pi}(s')]\n",
    "\\end{align*}\n",
    "\\begin{align*}\n",
    "q_{\\pi}(11, \\text{down}) = -1 + 1\\times0 = -1\n",
    "\\end{align*}\n",
    "\\begin{align*}\n",
    "q_{\\pi}(7, \\text{down}) = -1 + 1\\times(-14) = -15\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4.2 In Example 4.1, suppose a new state 15 is added to the gridworld just below state 13, and its actions, left, up, right, and down, take the agent to states 12, 13, 14, and 15, respectively. Assume that the transitions from the original states are unchanged. What, then, is $v_{\\pi}(15)$ for the equiprobable random policy? Now suppose the dynamics of state 13 are also changed, such that action down from state 13 takes the agent to the new state 15. What is $v_{\\pi}(15)$ for the equiprobable random policy in this case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- unchanged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "v_0(15) = 0\n",
    "\\end{align*}\n",
    "\\begin{align*}\n",
    "v_{1}(15) = 0.25 \\times [(-21) + (-23) + (-15) + (-1)] = -15.0\n",
    "\\end{align*}\n",
    "\\begin{align*}\n",
    "v_{2}(15) = 0.25 \\times [(-21) + (-23) + (-15) + (-16)] = -18.75\n",
    "\\end{align*}\n",
    "\\begin{align*}\n",
    "...\n",
    "\\end{align*}\n",
    "\\begin{align*}\n",
    "v_{\\infty}(15) = -20\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4.3 What are the equations analogous to (4.3), (4.4), and (4.5) for the action- valuefunction $q_{\\pi}$ and its successive approximation by a sequence of functions $q_0$,$q_1$,$q_2$,...?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "q_{k+1}(s, a) = \\sum_{s', r}^{} p(s',r{\\space}|s, a)[r(s,a,s') + {\\gamma}\\sum_{a'}^{}\\pi(a|s)q_{k}(s', a')]\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know $v_{\\pi}$, let's try to find better policies. We can choose an action a in some state s that is not in our policy and thereafter following the existing policy, $\\pi$ (we call this new policy $\\pi'$). If this choice is better than following $\\pi$ all the  time ($q_{\\pi}(s,\\pi'(s)) \\ge v_{\\pi}(s)$) than we can say that this new policy obtain greater or equal expected return from all states $s \\in S$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 4.2 Jack’s Car Rental"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4.4 The policy iteration algorithm on page 80 has a subtle bug in that it may never terminate if the policy continually switches between two or more policies that are equally good. This is ok for pedagogy, but not for actual use. Modify the pseudocode so that convergence is guaranteed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4.5 How would policy iteration be defined for action values? Give a complete algorithm for computing q⇤, analogous to that on page 80 for computing v⇤. Please pay special attention to this exercise, because the ideas involved will be used throughout the rest of the book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4.6 Suppose you are restricted to considering only policies that are \"-soft, meaning that the probability of selecting each action in each state, s, is at least \"/|A(s)|. Describe qualitatively the changes that would be required in each of the steps 3, 2, and 1, in that order, of the policy iteration algorithm for v⇤ on page 80."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4.7 (programming) Write a program for policy iteration and re-solve Jack’s car rental problem with the following changes. One of Jack’s employees at the first location rides a bus home each night and lives near the second location. She is happy to shuttle one car to the second location for free. Each additional car still costs \\$2, as do all cars moved in the other direction. In addition, Jack has limited parking space at each location. If more than 10 cars are kept overnight at a location (after any moving of cars), then an additional cost of $4 must be incurred to use a second parking lot (independent of how many cars are kept there). These sorts of nonlinearities and arbitrary dynamics often occur in real problems and cannot easily be handled by optimization methods other than dynamic programming. To check your program, first replicate the results given for the original problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 4.3 Gambler’s Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4.8 Why does the optimal policy for the gambler’s problem have such a curious form? In particular, for capital of 50 it bets it all on one flip, but for capital of 51 it does not. Why is this a good policy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4.9 (programming) Implement value iteration for the gambler’s problem and solve it for ph = 0.25 and ph = 0.55. In programming, you may find it convenient to introduce two dummy states corresponding to termination with capital of 0 and 100, giving them values of 0 and 1 respectively. Show your results graphically, as in Figure 4.3. Are your results stable as ✓ ! 0?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4.10 What is the analog of the value iteration update (4.10) for action values, qk+1(s, a)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Asynchronous Dynamic Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generalized Policy Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Efficiency of Dynamic Programming"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
